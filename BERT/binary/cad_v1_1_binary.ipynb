{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db8c1242-c415-42ef-b2f1-8c598bebc26b",
   "metadata": {},
   "source": [
    "### loading the dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23d7c687-32af-4af7-a1e1-26ed306ddfa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-11T14:30:14.199916Z",
     "iopub.status.busy": "2024-03-11T14:30:14.199916Z",
     "iopub.status.idle": "2024-03-11T14:30:14.220051Z",
     "shell.execute_reply": "2024-03-11T14:30:14.216111Z",
     "shell.execute_reply.started": "2024-03-11T14:30:14.199916Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Define your cache directory \n",
    "#deleting this is needed for each time you're making a new dataset based on the level\n",
    "cache_dir = \"/root/.cache/huggingface/datasets/contextual_abuse_dataset3/default/1.0.0\"\n",
    "\n",
    "# Remove the directory if it exists\n",
    "if os.path.exists(cache_dir):\n",
    "    shutil.rmtree(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3a0be06-e1d7-44d9-80d9-ea4d1e322742",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-11T14:30:16.635283Z",
     "iopub.status.busy": "2024-03-11T14:30:16.634137Z",
     "iopub.status.idle": "2024-03-11T14:30:18.318178Z",
     "shell.execute_reply": "2024-03-11T14:30:18.313349Z",
     "shell.execute_reply.started": "2024-03-11T14:30:16.635229Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import contextual_abuse_dataset3\n",
    "from contextual_abuse_dataset3 import ContextualAbuseRedditDataset\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249831e3-671a-4492-893e-25021a6e173f",
   "metadata": {},
   "source": [
    "### training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fe9a7bc-7959-4c66-aa57-05702e132313",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-11T14:30:22.083519Z",
     "iopub.status.busy": "2024-03-11T14:30:22.081993Z",
     "iopub.status.idle": "2024-03-11T14:30:38.335943Z",
     "shell.execute_reply": "2024-03-11T14:30:38.333394Z",
     "shell.execute_reply.started": "2024-03-11T14:30:22.083412Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset contextual_abuse_dataset3/default to /root/.cache/huggingface/datasets/contextual_abuse_dataset3/default/1.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d139567ce474b30b3829a6a894deb77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce691d7e1c384e13ac4bfce5c57586fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "752417a35c554ecf9a82f9c84770932f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contextual_abuse_dataset3 downloaded and prepared to /root/.cache/huggingface/datasets/contextual_abuse_dataset3/default/1.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a93f7a7b8d482ab8d7e53418bda5d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "import pandas as pd\n",
    "\n",
    "def prepare_contextual_abuse_datasets(level):\n",
    "    # Instantiate the dataset builder for the specified level\n",
    "    dataset_builder = ContextualAbuseRedditDataset(level=level)\n",
    "    dataset_builder.download_and_prepare()\n",
    "    dataset = dataset_builder.as_dataset()\n",
    "\n",
    "    # Split the dataset into train, test, and validation parts\n",
    "    test_dataset = dataset[\"test\"]\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    validation_dataset = dataset[\"validation\"]\n",
    "\n",
    "    # Concatenate the train and validation datasets for all levels\n",
    "    #total_train_dataset = concatenate_datasets([train_dataset, validation_dataset])\n",
    "\n",
    "    # Convert the datasets to pandas DataFrames\n",
    "    df_train = pd.DataFrame(train_dataset)\n",
    "    df_test = pd.DataFrame(test_dataset)\n",
    "    df_validation = pd.DataFrame(validation_dataset)\n",
    "\n",
    "    return df_train, df_validation, df_test\n",
    "\n",
    "# To call the function for a specific level, e.g., level 3:\n",
    "df_train, df_validation, df_test = prepare_contextual_abuse_datasets(level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da50af86-b38a-4480-a407-1d19bd0e95d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-11T14:30:38.346093Z",
     "iopub.status.busy": "2024-03-11T14:30:38.344110Z",
     "iopub.status.idle": "2024-03-11T14:30:38.393491Z",
     "shell.execute_reply": "2024-03-11T14:30:38.392332Z",
     "shell.execute_reply.started": "2024-03-11T14:30:38.346093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    11199\n",
      "1     2385\n",
      "Name: label, dtype: int64\n",
      "0    2385\n",
      "1    2385\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_train['label'] = df_train['labels_info'].apply(lambda x: x['label'][0] if (isinstance(x['label'], list) and len(x['label']) > 0) else None)\n",
    "print(df_train.label.value_counts())\n",
    "\n",
    "# Now filter the DataFrame\n",
    "df_majority = df_train[df_train.label == 0]\n",
    "df_minority = df_train[df_train.label == 1]\n",
    "\n",
    "# Undersample the majority class\n",
    "df_majority_undersampled = df_majority.sample(len(df_minority), random_state=42)\n",
    "\n",
    "# Combine the minority class with the undersampled majority class\n",
    "df_balanced = pd.concat([df_majority_undersampled, df_minority])\n",
    "\n",
    "# Shuffle the balanced dataframe\n",
    "df_balanced_train = df_balanced.sample(frac=1, random_state=42)\n",
    "df_balanced_train.reset_index(drop=True, inplace=True)\n",
    "print(df_balanced_train.label.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c81a371c-c625-4f5e-848c-941258e4d76c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-11T14:31:02.726269Z",
     "iopub.status.busy": "2024-03-11T14:31:02.726269Z",
     "iopub.status.idle": "2024-03-11T14:31:02.770271Z",
     "shell.execute_reply": "2024-03-11T14:31:02.766989Z",
     "shell.execute_reply.started": "2024-03-11T14:31:02.726269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    3733\n",
      "1     793\n",
      "Name: label, dtype: int64\n",
      "0    793\n",
      "1    793\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_validation['label'] = df_validation['labels_info'].apply(lambda x: x['label'][0] if (isinstance(x['label'], list) and len(x['label']) > 0) else None)\n",
    "print(df_validation.label.value_counts())\n",
    "\n",
    "# Now filter the DataFrame\n",
    "df_majorit = df_validation[df_validation.label == 0]\n",
    "df_minorit = df_validation[df_validation.label == 1]\n",
    "\n",
    "# Undersample the majority class\n",
    "df_majority_undersamp = df_majorit.sample(len(df_minorit), random_state=42)\n",
    "\n",
    "# Combine the minority class with the undersampled majority class\n",
    "df_balanced_val = pd.concat([df_majority_undersamp, df_minorit])\n",
    "\n",
    "# Shuffle the balanced dataframe\n",
    "df_balanced_val = df_balanced_val.sample(frac=1, random_state=42)\n",
    "df_balanced_val.reset_index(drop=True, inplace=True)\n",
    "print(df_balanced_val.label.value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "310bd209-a50e-43b5-8a72-3423fed3cdd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-11T14:31:06.978145Z",
     "iopub.status.busy": "2024-03-11T14:31:06.976561Z",
     "iopub.status.idle": "2024-03-11T14:31:06.987547Z",
     "shell.execute_reply": "2024-03-11T14:31:06.986283Z",
     "shell.execute_reply.started": "2024-03-11T14:31:06.978075Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Speaker1: What? [linebreak]  [linebreak] But Tokyo has more Michelin Stars than New York and Paris combined.... [linebreak]  [linebreak] And he works with Asian chefs... [linebreak]  [linebreak] He'll likely hire very talented Asian chefs here too. [SEP]\",\n",
       " \"Speaker2: Why, if you don't mind me asking? [SEP] Speaker3: I actually enjoy Swedish accents [SEP] Speaker4: I despise my Swedish accent [SEP]\")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = df_train.iloc[548][\"text\"]\n",
    "parent_text = df_train.iloc[540][\"parent_text\"]\n",
    "text, parent_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba094101-c4bc-4138-8bce-61f595416e9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-11T13:36:19.875065Z",
     "iopub.status.busy": "2024-03-11T13:36:19.874462Z",
     "iopub.status.idle": "2024-03-11T13:36:20.009155Z",
     "shell.execute_reply": "2024-03-11T13:36:20.007659Z",
     "shell.execute_reply.started": "2024-03-11T13:36:19.875018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word count in 'text': 34.49475837381744\n",
      "Median word count in 'text': 18.0\n",
      "Maximum word count in 'text': 1857\n",
      "90th percentile word count in 'text': 71.0\n",
      "\n",
      "Average word count in 'parent_text': 59.540611949203104\n",
      "Median word count in 'parent_text': 29.0\n",
      "Maximum word count in 'parent_text': 2234\n",
      "90th percentile word count in 'parent_text': 142.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df_train, df_validation, and df_test are already defined as shown\n",
    "# Combine all datasets for analysis\n",
    "combined_df = pd.concat([df_balanced_train, df_balanced_val, df_test])\n",
    "\n",
    "# Function to calculate word count\n",
    "def word_count(text):\n",
    "    return len(str(text).split())\n",
    "\n",
    "# Apply the function to the 'text' and 'parent_text' columns\n",
    "combined_df['text_word_count'] = combined_df['text'].apply(word_count)\n",
    "combined_df['parent_text_word_count'] = combined_df['parent_text'].apply(word_count)\n",
    "\n",
    "# Calculate average, median, maximum, and 85th percentile word count\n",
    "avg_word_count_text = combined_df['text_word_count'].mean()\n",
    "median_word_count_text = combined_df['text_word_count'].median()\n",
    "max_word_count_text = combined_df['text_word_count'].max()\n",
    "percentile_85_word_count_text = combined_df['text_word_count'].quantile(0.90)\n",
    "\n",
    "avg_word_count_parent_text = combined_df['parent_text_word_count'].mean()\n",
    "median_word_count_parent_text = combined_df['parent_text_word_count'].median()\n",
    "max_word_count_parent_text = combined_df['parent_text_word_count'].max()\n",
    "percentile_90_word_count_parent_text = combined_df['parent_text_word_count'].quantile(0.90)\n",
    "\n",
    "print(\"Average word count in 'text':\", avg_word_count_text)\n",
    "print(\"Median word count in 'text':\", median_word_count_text)\n",
    "print(\"Maximum word count in 'text':\", max_word_count_text)\n",
    "print(\"90th percentile word count in 'text':\", percentile_85_word_count_text)\n",
    "\n",
    "print(\"\\nAverage word count in 'parent_text':\", avg_word_count_parent_text)\n",
    "print(\"Median word count in 'parent_text':\", median_word_count_parent_text)\n",
    "print(\"Maximum word count in 'parent_text':\", max_word_count_parent_text)\n",
    "print(\"90th percentile word count in 'parent_text':\", percentile_90_word_count_parent_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbffdf19-84a7-429d-873c-275dd878ac10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-11T14:31:13.826137Z",
     "iopub.status.busy": "2024-03-11T14:31:13.822788Z",
     "iopub.status.idle": "2024-03-11T14:31:13.841919Z",
     "shell.execute_reply": "2024-03-11T14:31:13.840550Z",
     "shell.execute_reply.started": "2024-03-11T14:31:13.826060Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "n_classes = df_balanced_train['labels_info'].apply(lambda x: max(x['label'])).max() + 1\n",
    "print (n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3405886e-b626-4d49-a992-ce987b83206a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-11T14:31:16.058563Z",
     "iopub.status.busy": "2024-03-11T14:31:16.056581Z",
     "iopub.status.idle": "2024-03-11T14:31:16.105554Z",
     "shell.execute_reply": "2024-03-11T14:31:16.104068Z",
     "shell.execute_reply.started": "2024-03-11T14:31:16.058488Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "def get_label(x):\n",
    "    try:\n",
    "        return x['label'][0]\n",
    "    except IndexError:\n",
    "        return None  # or some default value\n",
    "\n",
    "labels = df_train['labels_info'].apply(get_label).values\n",
    "\n",
    "import numpy as np\n",
    "df_train = df_train[df_train['labels_info'].apply(get_label).notna()]\n",
    "\n",
    "print(np.unique(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0ae348b-2a49-484e-a0db-d573e16b8e83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-11T14:31:21.248786Z",
     "iopub.status.busy": "2024-03-11T14:31:21.247813Z",
     "iopub.status.idle": "2024-03-11T14:31:38.882803Z",
     "shell.execute_reply": "2024-03-11T14:31:38.881088Z",
     "shell.execute_reply.started": "2024-03-11T14:31:21.248779Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "890707ee4a0f4eccaedf0bc94e88acc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b41bd6cd3ddb4d2aaf95ccb03d601dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368ba5fbc59741938d5594868a6c3aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer, Trainer, TrainingArguments, BertForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "import torch\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments,EarlyStoppingCallback, BertConfig\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import logging\n",
    "\n",
    "from transformers import BertModel, BertConfig, BertPreTrainedModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def get_torch_device(verbose: bool = True, gpu_ix: int = 0) -> torch.device:\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        if verbose:\n",
    "            print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "            print('We will use the GPU:', torch.cuda.get_device_name(gpu_ix))\n",
    "    else:\n",
    "        if verbose: print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device\n",
    "\n",
    "def save_model(output_dir:str, model, tokenizer):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    print(\"Saving model to %s\" % output_dir)\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "    model_to_save.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.data.iloc[index].text\n",
    "        parent_text = self.data.iloc[index].parent_text\n",
    "        # Adjusted to extract the first label from the nested structure\n",
    "        label = torch.tensor([self.data.iloc[index]['labels_info']['label'][0]]).float()\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text + ' [SEP] ' + parent_text,  # Combining text and parent_text with [SEP] token\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': label  # Ensure this matches the model's expected label format\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "class CustomBertForSequenceClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = 1\n",
    "        self.context_bert = BertModel(config)\n",
    "        self.text_bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.text_attention = nn.Linear(config.hidden_size, 1)\n",
    "        self.parent_text_attention = nn.Linear(config.hidden_size, 1)\n",
    "        self.classifier = nn.Linear(config.hidden_size * 3, self.num_labels)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        # Apply separate attention mechanisms for text and parent text\n",
    "        context_outputs = self.context_bert(input_ids, attention_mask=attention_mask)\n",
    "        text_outputs = self.text_bert(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        context_pooled_output = context_outputs[1]\n",
    "        text_pooled_output = text_outputs[1]\n",
    "\n",
    "        context_pooled_output = self.dropout(context_pooled_output)\n",
    "        text_pooled_output = self.dropout(text_pooled_output)\n",
    "\n",
    "        text_attention_scores = self.text_attention(text_outputs[0])\n",
    "        parent_text_attention_scores = self.parent_text_attention(context_outputs[0])\n",
    "\n",
    "        text_attention_scores = torch.softmax(text_attention_scores, dim=1)\n",
    "        parent_text_attention_scores = torch.softmax(parent_text_attention_scores, dim=1)\n",
    "\n",
    "        text_attended_output = torch.sum(text_attention_scores * text_outputs[0], dim=1)\n",
    "        parent_text_attended_output = torch.sum(parent_text_attention_scores * context_outputs[0], dim=1)\n",
    "\n",
    "        # Combine the attended outputs with different weights\n",
    "        text_weight = 0.7\n",
    "        parent_text_weight = 0.3\n",
    "        combined_output = text_weight * text_attended_output + parent_text_weight * parent_text_attended_output\n",
    "\n",
    "        # Concatenate the pooled output and combined attended output\n",
    "        final_output = torch.cat((context_pooled_output, text_pooled_output, combined_output), dim=-1)\n",
    "\n",
    "        logits = self.classifier(final_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            return loss, logits\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Assuming you have extended CustomBertForSequenceClassification correctly\n",
    "# and it accepts the same initialization arguments as BertModel.\n",
    "config = BertConfig.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model = CustomBertForSequenceClassification(config)\n",
    "\n",
    "# Define the datasets\n",
    "df_train1 = CustomDataset(df_balanced_train, tokenizer, max_len=400)\n",
    "df_val1 = CustomDataset(df_balanced_val, tokenizer, max_len=400)\n",
    "\n",
    "# Define the compute_metrics function\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = torch.sigmoid(torch.tensor(pred.predictions))\n",
    "    preds = (preds > 0.5).int()\n",
    "\n",
    "    # Now labels and preds are in the same format, we can compute the metrics\n",
    "    precision = precision_score(labels, preds, average='weighted')\n",
    "    recall = recall_score(labels, preds, average='weighted')\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Define the training arguments\n",
    "\n",
    "#level 1 best result with 0.0000008 and 9 epochs.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',  # Add this line to set the save strategy\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.03,\n",
    "    learning_rate=0.00002, \n",
    "    save_total_limit=3,\n",
    "    logging_dir='./logs',\n",
    "    load_best_model_at_end=True,\n",
    "    lr_scheduler_type='cosine_with_restarts',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ef00627-255e-4bf3-8af2-525e67f74273",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-11T14:31:40.206557Z",
     "iopub.status.busy": "2024-03-11T14:31:40.204335Z",
     "iopub.status.idle": "2024-03-11T14:31:45.399921Z",
     "shell.execute_reply": "2024-03-11T14:31:45.397944Z",
     "shell.execute_reply.started": "2024-03-11T14:31:40.206557Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create an instance of the EarlyStoppingCallback\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=2) \n",
    "# Create the Trainer and train the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=df_train1,\n",
    "    eval_dataset=df_val1,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca0e5f35-60e7-4d65-ba7b-8e08492efacd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-11T14:31:48.587173Z",
     "iopub.status.busy": "2024-03-11T14:31:48.586517Z",
     "iopub.status.idle": "2024-03-11T15:06:16.135841Z",
     "shell.execute_reply": "2024-03-11T15:06:16.130305Z",
     "shell.execute_reply.started": "2024-03-11T14:31:48.587110Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4770\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 897\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/wandb/run-20240311_143208-2qvuld2g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fa-payam/huggingface/runs/2qvuld2g\" target=\"_blank\">./results</a></strong> to <a href=\"https://wandb.ai/fa-payam/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='897' max='897' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [897/897 34:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.698657</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.737800</td>\n",
       "      <td>0.696039</td>\n",
       "      <td>0.578815</td>\n",
       "      <td>0.576224</td>\n",
       "      <td>0.580790</td>\n",
       "      <td>0.578815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.737800</td>\n",
       "      <td>0.703206</td>\n",
       "      <td>0.597100</td>\n",
       "      <td>0.596422</td>\n",
       "      <td>0.597756</td>\n",
       "      <td>0.597100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1586\n",
      "  Batch size = 16\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-299\n",
      "Configuration saved in ./results/checkpoint-299/config.json\n",
      "Model weights saved in ./results/checkpoint-299/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-302] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1586\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-598\n",
      "Configuration saved in ./results/checkpoint-598/config.json\n",
      "Model weights saved in ./results/checkpoint-598/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-604] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1586\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-897\n",
      "Configuration saved in ./results/checkpoint-897/config.json\n",
      "Model weights saved in ./results/checkpoint-897/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-906] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-598 (score: 0.696039080619812).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=897, training_loss=0.6968893776237632, metrics={'train_runtime': 2067.4462, 'train_samples_per_second': 6.922, 'train_steps_per_second': 0.434, 'total_flos': 5883025095288000.0, 'train_loss': 0.6968893776237632, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d12603d8-d2a7-473d-991f-b5ab159ac352",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-11T15:14:34.615858Z",
     "iopub.status.busy": "2024-03-11T15:14:34.614187Z",
     "iopub.status.idle": "2024-03-11T15:14:34.624053Z",
     "shell.execute_reply": "2024-03-11T15:14:34.620923Z",
     "shell.execute_reply.started": "2024-03-11T15:14:34.615800Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test1 = CustomDataset(df_test, tokenizer, max_len=400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e52c57d-40cd-44bc-bf58-d123897ffa56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-11T15:14:37.703970Z",
     "iopub.status.busy": "2024-03-11T15:14:37.702653Z",
     "iopub.status.idle": "2024-03-11T15:18:51.062948Z",
     "shell.execute_reply": "2024-03-11T15:18:51.061653Z",
     "shell.execute_reply.started": "2024-03-11T15:14:37.703919Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 5307\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='419' max='332' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [332/332 05:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_results = trainer.evaluate(df_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae2ae151-efab-4437-82dd-2e5c96fecdb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-11T15:21:46.876507Z",
     "iopub.status.busy": "2024-03-11T15:21:46.875787Z",
     "iopub.status.idle": "2024-03-11T15:21:46.914858Z",
     "shell.execute_reply": "2024-03-11T15:21:46.913654Z",
     "shell.execute_reply.started": "2024-03-11T15:21:46.876410Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Use Trainer to predict\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(df_test1)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# Use Trainer to predict\n",
    "predictions = trainer.predict(df_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df93dc9d-b88b-4788-864b-a817930b04bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T15:31:10.686578Z",
     "iopub.status.busy": "2024-01-16T15:31:10.686296Z",
     "iopub.status.idle": "2024-01-16T15:31:10.707701Z",
     "shell.execute_reply": "2024-01-16T15:31:10.707066Z",
     "shell.execute_reply.started": "2024-01-16T15:31:10.686557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.76      0.83      4410\n",
      "           1       0.45      0.58      0.51       897\n",
      "\n",
      "   micro avg       0.81      0.73      0.77      5307\n",
      "   macro avg       0.69      0.67      0.67      5307\n",
      "weighted avg       0.85      0.73      0.78      5307\n",
      " samples avg       0.73      0.73      0.73      5307\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#02 #level 1\n",
    "from sklearn.preprocessing import binarize  \n",
    "from sklearn.metrics import classification_report\n",
    "binary_predictions = binarize(predictions.predictions, threshold=0.5)\n",
    "\n",
    "# Extract true labels\n",
    "true_labels = predictions.label_ids \n",
    "\n",
    "print(classification_report(true_labels, binary_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6198347f-c590-4e6d-8bd1-aae27b2e8ae5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T09:16:24.521183Z",
     "iopub.status.busy": "2024-01-16T09:16:24.520865Z",
     "iopub.status.idle": "2024-01-16T09:16:24.543746Z",
     "shell.execute_reply": "2024-01-16T09:16:24.542737Z",
     "shell.execute_reply.started": "2024-01-16T09:16:24.521160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.70      0.80      4401\n",
      "           1       0.41      0.67      0.51       906\n",
      "\n",
      "   micro avg       0.77      0.69      0.73      5307\n",
      "   macro avg       0.67      0.68      0.65      5307\n",
      "weighted avg       0.85      0.69      0.75      5307\n",
      " samples avg       0.69      0.69      0.69      5307\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#02 #level 2\n",
    "from sklearn.preprocessing import binarize  \n",
    "from sklearn.metrics import classification_report\n",
    "binary_predictions = binarize(predictions.predictions, threshold=0.5)\n",
    "\n",
    "# Extract true labels\n",
    "true_labels = predictions.label_ids \n",
    "\n",
    "print(classification_report(true_labels, binary_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "233cdff3-0d5b-4347-bbde-1ce9f6ca53f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T09:26:45.578947Z",
     "iopub.status.busy": "2024-01-16T09:26:45.578241Z",
     "iopub.status.idle": "2024-01-16T09:26:45.595768Z",
     "shell.execute_reply": "2024-01-16T09:26:45.595338Z",
     "shell.execute_reply.started": "2024-01-16T09:26:45.578919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.69      0.80      4401\n",
      "           1       0.41      0.67      0.51       906\n",
      "\n",
      "   micro avg       0.77      0.69      0.73      5307\n",
      "   macro avg       0.67      0.68      0.65      5307\n",
      "weighted avg       0.85      0.69      0.75      5307\n",
      " samples avg       0.69      0.69      0.69      5307\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#02 #level 3 baseline conctanting text\n",
    "from sklearn.preprocessing import binarize  \n",
    "from sklearn.metrics import classification_report\n",
    "binary_predictions = binarize(predictions.predictions, threshold=0.5)\n",
    "\n",
    "# Extract true labels\n",
    "true_labels = predictions.label_ids \n",
    "\n",
    "print(classification_report(true_labels, binary_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3b6a7f38-ee57-4f02-b8d5-6177531a0dec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-11T11:50:14.321939Z",
     "iopub.status.busy": "2024-03-11T11:50:14.321222Z",
     "iopub.status.idle": "2024-03-11T11:50:14.348092Z",
     "shell.execute_reply": "2024-03-11T11:50:14.346943Z",
     "shell.execute_reply.started": "2024-03-11T11:50:14.321899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.79      0.83      4401\n",
      "           1       0.32      0.48      0.38       906\n",
      "\n",
      "    accuracy                           0.73      5307\n",
      "   macro avg       0.60      0.63      0.61      5307\n",
      "weighted avg       0.78      0.73      0.75      5307\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#experiment with two bert models one for context one for text\n",
    "from sklearn.preprocessing import binarize  \n",
    "from sklearn.metrics import classification_report\n",
    "binary_predictions = binarize(predictions.predictions, threshold=0.5)\n",
    "\n",
    "# Extract true labels\n",
    "true_labels = predictions.label_ids \n",
    "\n",
    "print(classification_report(true_labels, binary_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5f75bd41-e62f-484f-a60a-9eb8cd9573bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-11T11:50:14.354816Z",
     "iopub.status.busy": "2024-03-11T11:50:14.354064Z",
     "iopub.status.idle": "2024-03-11T11:50:14.385198Z",
     "shell.execute_reply": "2024-03-11T11:50:14.383727Z",
     "shell.execute_reply.started": "2024-03-11T11:50:14.354774Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of converted_predictions: 5307\n",
      "Number of rows in df_test: 5307\n"
     ]
    }
   ],
   "source": [
    "def get_label_map():\n",
    "    label_map = {'Neutral': 0, 'Abusive Speech': 1}\n",
    "    inv_label_map = {v: k for k, v in label_map.items()}\n",
    "    return label_map, inv_label_map\n",
    "\n",
    "# Step 1: Map binary predictions back to original categories\n",
    " #Retrieve the inverse label map\n",
    "_, inv_label_map = get_label_map()\n",
    "\n",
    "# Convert numeric predictions to string labels\n",
    "converted_predictions = [inv_label_map[label] for label in binary_predictions.flatten()]\n",
    "print(f\"Length of converted_predictions: {len(converted_predictions)}\")\n",
    "print(f\"Number of rows in df_test: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c25c46ba-97e5-40b5-89b7-2ee4ea48d416",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-11T11:50:14.388375Z",
     "iopub.status.busy": "2024-03-11T11:50:14.387428Z",
     "iopub.status.idle": "2024-03-11T11:50:14.414330Z",
     "shell.execute_reply": "2024-03-11T11:50:14.412708Z",
     "shell.execute_reply.started": "2024-03-11T11:50:14.388318Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming predictions are obtained from the Trainer\n",
    "preds = predictions.predictions\n",
    "labels = predictions.label_ids\n",
    "\n",
    "# Convert predictions to label names\n",
    "_, inv_label_map = get_label_map()\n",
    "processed_preds = [inv_label_map[label] for label in np.argmax(preds, axis=1)]\n",
    "\n",
    "# Extract relevant columns from df_test1 for analysis\n",
    "df_analysis = df_test[['text', 'parent_text', 'labels_info']].copy()\n",
    "\n",
    "# Add predictions to the DataFrame with specific naming\n",
    "level = 3  # Set the level appropriately\n",
    "df_analysis[f'prediction_level_{level}'] = processed_preds\n",
    "\n",
    "# Convert labels_info from numeric to label names\n",
    "df_analysis['labels_info'] = df_analysis['labels_info'].apply(lambda x: inv_label_map[x['label'][0]])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "#df_analysis.to_csv(f'error_analysis_level_{level}.csv', index=False)\n",
    "\n",
    "# For adding predictions from other levels later, you can read this file and add new columns accordingly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0c2d138b-51d1-4c75-94c4-982471f329d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-11T11:50:14.417267Z",
     "iopub.status.busy": "2024-03-11T11:50:14.416363Z",
     "iopub.status.idle": "2024-03-11T11:50:14.535671Z",
     "shell.execute_reply": "2024-03-11T11:50:14.534358Z",
     "shell.execute_reply.started": "2024-03-11T11:50:14.417212Z"
    }
   },
   "outputs": [],
   "source": [
    "df_analysis.to_csv(f'error_analysis_level_{level}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
